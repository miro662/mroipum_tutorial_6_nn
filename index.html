<!DOCTYPE html>
<html lang="pl">
<head>
<meta charset="utf-8"/>
<title>Sieci Neuronowe</title>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/reveal.css"/>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/theme/black.css" id="theme"/>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/css/zenburn.css"/>

<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'https://cdn.jsdelivr.net/reveal.js/3.0.0/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide">
<h1 class="title">Sieci Neuronowe</h1>
</section>

<section>
<section id="slide-orgc685d4a">
<h2 id="orgc685d4a">Neuron</h2>
<ul>
<li>Struktura wzorowana na budowie biologicznego neuronu</li>
<li>Dowolna liczba wejść z różną wagą</li>
<li>Jedno wyjście</li>

</ul>

<p>
Wyjście to wynik funkcji aktywacji dla sumy ważonej wejść:
</p>

<p>
\(Y = f(\sum_{i=1}^{n} W_i*X_i)\)
</p>


<div class="figure">
<p><img src="./images/1a.png" alt="1a.png" width="600px" />
</p>
</div>

</section>
<section id="slide-org12de099">
<h3 id="org12de099">Funkcja aktywacji</h3>
<p>
Powinna być nieliniowa, aby wyjście nie było kombinacją liniową wejść.
<img src="./images/1b.png" alt="1b.png" />
</p>

</section>
</section>
<section>
<section id="slide-org3a023c0">
<h2 id="org3a023c0">Multi Layer Perceptron - Budowa</h2>
<p>
Budowa MLP polega na zgrupowaniu neuronów w warstwy.
</p>

<ul>
<li>Wszystkie neurony w jednej warstwie mają tyle samo wejść.</li>
<li>Wyjście neuronu jest wejściem dla każdego neuronu w kolejnej warstwie.</li>

</ul>

<p>
\(WC_i\) - Liczba wag w warstwie \(i\)
</p>

<p>
\(N_i\) - liczba neuronów w warstwie i
</p>

<p>
\(WC_i\) = \(N_i*N_{i-1}\) dla \(i\gt1\)
</p>

<p>
\(WC_i\) = \(N_i\) dla \(i=1\)
</p>

</section>
<section id="slide-orga124ada">
<h3 id="orga124ada"></h3>

<div class="figure">
<p><img src="./images/2a.png" alt="2a.png" />
</p>
</div>

</section>
<section id="slide-orgee09983">
<h3 id="orgee09983">Backpropagation</h3>
<p>
Celem szkolenia jest zminimaliozwanie funkcji kosztu:
\(C(w_1, w_2, ..., w_M) =\) np. błąd średniokwadratowy
</p>

<p>
Wyznaczenie gradientu  \(\nabla C = [\frac{dC}{dw_1}, \frac{dC}{dw_2}, ..., \frac{dC}{dw_M}]\)
pozwoli na dostosowanie wag i zmniejszenie błędu.
</p>

<p>
Aby wyznaczyć elementy wektora \(\nabla C\) stosuje się algorytm <code>propagacji wstecznej</code>. 
</p>

<p>
Konceptualnie jest to po prostu liczenie pochodnych po różnych wagach przy użyciu <code>reguły łańcuchowej</code>.
</p>
</section>
<section id="slide-org81f4e94">
<h3 id="org81f4e94">Przykłady</h3>
<ul>
<li>\(x\) i \(y\) to stałe wektory o równych wymiarach</li>
<li>\(C(y, f^N(W^Nf^{N-1}(W^{N-1}...f^1(W^x))))\) to funkcja kosztu, zmiennymi są wagi</li>
<li>\(W^i\) to macierz wag w i-tej warstwie</li>
<li>\(W^i_j\) to wektor wag wejściowych w i-tej warstwie dla j-tego neuronu</li>
<li>\(z^i_j\) to nieaktywowane wyjście j-tego neuronu w i-tej warstwie</li>
<li>\(a^i_j = f(z^i_j)\)</li>

</ul>

<p>
Przykłady:
</p>
<ul>
<li>\(\frac{dC}{dW^N_j} = \frac{dC}{da^N}\frac{da^N}{dz^N}\frac{dz^N}{dW^N_j}\)</li>
<li>\(\frac{dC}{dW^{N-1}_j} = \frac{dC}{da^N}\frac{da^N}{dz^N}\frac{dz^N}{da^{N-1}}\frac{da^{N-1}}{dz^{N-1}}\frac{dz^{N-1}}{dW^{N-1}_j}\)</li>

</ul>

</section>
<section id="slide-orgb4a6c52">
<h3 id="orgb4a6c52">Vanishing gradient problem</h3>
<ul>
<li>Wartości gradientów w pierwszych warstawch sieci osiągają bardzo małe wartości, przez co prawie się nie zmieniają.</li>
<li>Wynika to z tego, że podczas liczenia pochodnych z użyciem reguły łańcuchowej mnożymy wiele małych wartości.</li>
<li><p>
Rozwiązania: Batch Normalization, użycie funkcji aktywacji, które nie skutkują małymi pochodnymi.
</p>


<div class="figure">
<p><img src="./images/2b.jpg" alt="2b.jpg" />
</p>
</div></li>

</ul>

</section>
</section>
<section>
<section id="slide-org597f1b8">
<h2 id="org597f1b8">Metody uczenia</h2>
<ul>
<li><b>SGD</b> - Stochastic Gradient Descent
<ul>
<li>Gradient jest liczony dla każdej próbki z osobna, a nie całego zbioru danych na raz</li>
<li>Przyspiesza czas szkolenia</li>

</ul></li>
<li><b>SGD with momentum</b> 
<ul>
<li>Przy liczeniu gradientu brana jest pod uwagę uśredniona wartość z wielu poprzednich aktualizacji lub tylko poprzednia aktualizacja.</li>
<li>Nie modyfikujemy stałej uczenia \(\alpha\)</li>
<li>Parametr \(\beta\) określa wagę poprzednich aktualizacji</li>
<li>Nowa wartość: 
<ul>
<li>\(j\) - indeks wagi</li>
<li>\(i\) - iteracja</li>
<li>\(\Delta w_j^{i}=\beta \Delta w_{j}^{i-1}+\alpha \frac{dC}{dw_j}\)</li>

</ul></li>

</ul></li>

</ul>
</section>
<section id="slide-orgdc86bfd">
<h3 id="orgdc86bfd"></h3>
<ul>
<li><p>
<b>RMSProp</b>
</p>
<ul>
<li>Wykorzystuje średnią z przeszłych gradientów do dostoswanie stałej uczenia \(\alpha\)</li>
<li>Nowa wartość:
<ul>
<li>\(v_j\) - średnia kwadratowa poprzednich gradientów</li>
<li>\(\Delta w_j=\frac{\alpha}{\sqrt{v_j}}\frac{dC}{dw_j}\)</li>

</ul></li>

</ul>
<ul>
<li><b>ADAM</b>
<ul>
<li>Połączenie <b>RMSProp</b> i <b>SGD with momentum</b></li>
<li>Uwzględniamy poprzednie aktualizacje oraz dostosowujemy stałą uczenia \(\alpha\)</li>

</ul></li>
<li><b>Nestorov</b>
<ul>
<li>Podobna do <b>SGD with momentum</b></li>
<li>Wagi są aktualizowane zgodnie z pędem i dopiero wtedy liczony jest kolejny gradient</li>

</ul></li>

</ul></li>

</ul>

</section>
</section>
<section>
<section id="slide-org8375599">
<h2 id="org8375599">Loss functions</h2>
<ul>
<li>Klasyfiaktory binarne
<ul>
<li><p>
Binary Cross-Entropy
</p>


<div class="figure">
<p><img src="./images/4a.png" alt="4a.png" />
</p>
</div></li>
<li><p>
Hinge loss
</p>


<div class="figure">
<p><img src="./images/4c.png" alt="4c.png" />
</p>
</div></li>
<li><p>
Squared Hinge Loss
</p>


<div class="figure">
<p><img src="./images/4d.png" alt="4d.png" />
</p>
</div></li>

</ul></li>

</ul>
</section>
<section id="slide-org49e1a04">
<h3 id="org49e1a04"></h3>
<ul>
<li><p>
Klasyfikatory wieloklasowe
</p>
<ul>
<li><p>
Categorical Cross-Entropy
</p>


<div class="figure">
<p><img src="./images/4b.png" alt="4b.png" />
</p>
</div></li>
<li>Kullback-Leibler divergence(relative entropy)</li>

</ul>

<p>
Pozwala na porónwanie dwóch rozkładów prawdopodobieństwa.
</p>


<div class="figure">
<p><img src="./images/4e.png" alt="4e.png" />
</p>
</div></li>

</ul>
</section>
</section>
<section>
<section id="slide-org688f0c1">
<h2 id="org688f0c1">Inicjalizacja wag</h2>
<ul>
<li>Dobra inicjalizacja wag pomaga zredukować <b>Vanishing Gradient Problem</b>.</li>
<li>Kilka ważnych cech:
<ul>
<li>Wagi nie powinny być małe</li>
<li>Wagi nie powinny być takie same</li>
<li>Wagi powinny mieć wysoką wariancję</li>
<li>Wybór metody inicjalizacja zależny od użytych funkcji aktywacji</li>
<li>Pod uwagę brana ilość wejść/wyjść w wartswie.</li>

</ul></li>

</ul>

<div class="figure">
<p><img src="./images/5a.png" alt="5a.png" />
</p>
</div>

</section>
<section id="slide-org3b09434">
<h3 id="org3b09434">Stała uczenia</h3>
<ul>
<li>Określa jak duży krok wykonywany jest przy aktualizacji wag.</li>
<li><b>Learning rate decay</b> - wraz z liczbą epok zmniejsza się stałą uczenia.
<ul>
<li>Pozwala na zmniejszenie szansy na 'przeskoczenie' minimum</li>
<li>Przypomina <b>simulated annealing</b></li>

</ul></li>

</ul>


<div class="figure">
<p><img src="./images/5b.png" alt="5b.png" />
</p>
</div>

</section>
<section id="slide-orgddc21b9">
<h3 id="orgddc21b9">Batch normalization</h3>
<ul>
<li>Normalizacja polega na przeskalowaniu cech na ten sam przedział. Najczęście na [0; 1].</li>
<li><b>Batch normalization</b> działa tak samo jak normalizacja danych wejściowych, z tym że znajduje się pomiędzy warstwami.</li>
<li>Co daje <b>batch normalization</b>?
<ul>
<li>Przestrzeń wejść jest bardziej symetryczna - można używać wyższych stałych uczenia</li>
<li>Zmniejsza istotność wag początkowych.</li>

</ul></li>

</ul>


<div class="figure">
<p><img src="./images/5c.png" alt="5c.png" width="500px" />
</p>
</div>
</section>
</section>
<section>
<section id="slide-orgd815d68">
<h2 id="orgd815d68">Regularyzacja</h2>
<ul>
<li>Celem regularyzacji jest zmniejszenie overfittingu.</li>
<li>Innymi słowy: staramy się zwiększyć bias i zmniejszyć wariancję na zbiorze treningowym.</li>
<li>Poświęcamy dokładność na zbiorze treningowym licząc na otrzymanie lepiej generalizującego modelu.</li>

</ul>
</section>
<section id="slide-org9e2954a">
<h3 id="org9e2954a"></h3>
<ul>
<li><p>
Regularyzacja <b>Dropout</b>
</p>

<p>
Przypisujemy dla warstwy pewne prawdopodobieństwo pominięcia neuronu.
</p>

<p>
Pominięte neurony nie biorą udziału w danej iteracji <b>gradient descent</b>.
</p></li>

<li><p>
Regularyzacja <b>L1</b> i <b>L2</b>
Polegają na dodaniu pewnego wyrażenia regularyzującego do funkcji kosztu.
</p>

<div class="figure">
<p><img src="./images/6a.png" alt="6a.png" width="600px" />
</p>
</div></li>

</ul>
</section>
<section id="slide-org8121d19">
<h3 id="org8121d19"></h3>
<ul>
<li>Regularyzacja przez <b>augmentację</b>
<ul>
<li>Polega na wygenerowaniu nowych danych treningowych na podstawie już posiadanych.</li>
<li>Zwiększanie ilości danych zmniejsza wariancję i zwiększa bias.</li>

</ul></li>

</ul>

</section>
</section>
<section>
<section id="slide-orgc434a3f">
<h2 id="orgc434a3f">Sieci CNN - budowa</h2>
<ul>
<li>Sieci CCN składają się z kilku następujących typów warstw:
<ul>
<li>Warstwy konwolucyjne</li>
<li>Warstwy poolingu</li>
<li>Warstwy w pełni połączone - takie same jak w MLP</li>

</ul></li>

</ul>


<div class="figure">
<p><img src="./images/7a.png" alt="7a.png" />
</p>
</div>

</section>
<section id="slide-org00f9a15">
<h3 id="org00f9a15">Warstwy konwolucyjne</h3>
<ul>
<li>Wykonują proces <b>konwolucji</b>, polegający na, aplikacji filtra na pewnym framencie danych wejściowych.</li>
<li>Aplikacja filtra polega na obliczeniu <b>dot productu</b> między filtrem a fragmentem danych.</li>
<li>Parametry:
<ul>
<li><b>stride</b> - pozwala na zmianę sposobu przesuwania filtra</li>
<li><b>padding</b> - pozwala na dodanie zer wokół, aby uniknąć zmniejszania utraty wymiarów</li>

</ul></li>

</ul>


<div class="figure">
<p><img src="./images/7b.png" alt="7b.png" width="550px" />
</p>
</div>

</section>
<section id="slide-org880bcf1">
<h3 id="org880bcf1"></h3>
<ul>
<li>W warstwach konwolucyjnych zwykle znajduje się więcej niż jeden filtr.
<img src="./images/7d.png" alt="7d.png" /></li>

</ul>
</section>
<section id="slide-org1927977">
<h3 id="org1927977">Warstwy poolingu</h3>
<ul>
<li>Pozwala na <b>downsampling</b> wejść.</li>
<li>Parametr <b>filter</b> pozwala na wybór wielkości okna do uśrednienia.</li>
<li>Parametr <b>stride</b> pozwala określić jak wygląda przesuwanie okna.</li>
<li>Dwie popularne sposoby poolingu:
<ul>
<li>Average pooling - uśredniona wartość z okna</li>
<li>Maximum pooling - maksymalna wartość z okna</li>

</ul></li>

</ul>


<div class="figure">
<p><img src="./images/7c.png" alt="7c.png" width="600px" />       
</p>
</div>

</section>
<section id="slide-org9742ff3">
<h3 id="org9742ff3">Architektury</h3>
<ul>
<li>AlexNet - 2012 - 5 warstw konwolucyjnych, 3 warstwy MLP, 60 milionów parametrów. Jako pierwsza używała funkcji ReLU do aktywacji.</li>
<li>Inception - 2014, 2015, 2016 - Wprowadza ideę budowania sieci na bazie modulów. Wewnątrz modułu może być wiele równoległych wartsw konwolucyjnych. Moduły nazwano 'Inception modules'.</li>
<li>Residual neural network - 2015, 2017 - wprowadza tak zwane <code>skip connections</code>, czyli połączenia pomiędzy nie sąsiadującymi warstwami.</li>

</ul>

</section>
</section>
<section>
<section id="slide-org638038e">
<h2 id="org638038e">Sieci CNN - uczenie</h2>
<ul>
<li>W przypadku warstw końcowych trening wygląda identycznie jak w MLP.</li>
<li>Warstwy z poolingiem nie posiadają parametrów do trenowania.</li>
<li>W warstwach konwolucyjnych wagami są wartości filtrów.</li>
<li>Ideowo backpropagation oraz dostosowywanie wag działa jak w MLP.</li>
<li>Przy liczeniu pochodnych trzeba uwzględnić warstwy poolingu.</li>

</ul>
</section>
</section>
<section>
<section id="slide-orgf71500a">
<h2 id="orgf71500a">Scikitlearn</h2>
<ul>
<li>Wsparcie dla MLP:
<ul>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html">sklearn.neural​_network.MLPClassifier</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html">sklearn.neural​_network.MLPRegressor</a></li>

</ul></li>
<li>API identycznie jak dla pozostałych modeli: <code>fit(X, y)</code>, <code>predict(x)</code>, etc&#x2026;</li>
<li>Można dostosować wielkości ukrytych wartsw, funkcje aktywacji, solvery i inne hiperparametry.</li>
<li>Brak wsparcia dla akceleracji GPU.</li>
<li>Brak wsparcia dla CNN.</li>
<li>Nie nadaje się do produkcyjnych zastosowań.</li>

</ul>
</section>
</section>
<section>
<section id="slide-org426b9f1">
<h2 id="org426b9f1">Keras</h2>
<ul>
<li>Wysokopoziomowa biblioteka umożliwiająca łatwe definiowanie sieci neuronowych.</li>
<li>Posiada wsparcie dla większości popularnych typów warstw.</li>
<li>Pozwala na tworzenie własnych typów warstw.</li>
<li>Umożliwiającą akcelerację GPU, wspiera backendy dla kart Nvidia i AMD.</li>
<li>Kompatybilny z API Scikitlearn, nakładka <a href="https://keras.io/scikit-learn-api/">KerasClassifier</a></li>

</ul>


<div class="figure">
<p><img src="./images/9.png" alt="9.png" width="600px" />
</p>
</div>

</section>
</section>
<section>
<section id="slide-org151bf1a">
<h2 id="org151bf1a">PyTorch</h2>
<ul>
<li>W porównaniu do Kerasa dość niskopoziomowa.</li>
<li>Bardziej elastycznia niż Keras, ale trudniejsza w użyciu.</li>
<li><b>PyTorch Tensors</b> -  działają one podobnie jak tablice w numpy, ale operacje na nich są akcelerowane za pomocą GPU.</li>
<li>Wspiera akcelerację GPU, ale tylko dla kart Nvidia z rdzeniami CUDA.</li>
<li>Wspiera Javę i C++</li>

</ul>
</section>
</section>
<section>
<section id="slide-orga5ede9c">
<h2 id="orga5ede9c">Dziękujemy</h2>
<div class="outline-text-2" id="text-orga5ede9c">
</div>
</section>
<section id="slide-orga71da5e">
<h3 id="orga71da5e">Zespół</h3>
<ul>
<li>Andrzej Ratajczak</li>
<li>Damian Wasilenko</li>
<li>Dawid Macek</li>
<li>Mirosław Błażej</li>

</ul>
</section>
</section>
</div>
</div>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/head.min.js"></script>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/js/reveal.js"></script>
<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c',
rollingLinks: false,
keyboard: true,
mouseWheel: false,
fragmentInURL: false,
hashOneBasedIndex: false,
pdfSeparateFragments: true,

overview: true,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'convex', // see README of reveal.js for options
transitionSpeed: 'default',

// Optional libraries used to extend reveal.js
dependencies: [
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }]

});
</script>
</body>
</html>
