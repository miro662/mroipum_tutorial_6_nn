{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 6 - Sieci neuronowe\n",
    "\n",
    "## 1) Sieci MLP - multi-layer perceptron\n",
    "### 1.1) Zasada działania\n",
    "\n",
    "MLP (perceptron wielowarstwowy) jest algorytmem _uczenia nadzorowanego_. Dla danego zbioru wektorów cech X oraz wektora odpowiedzi y trenuje on _nieliniową_ fukcję $f: R^m \\rightarrow R^o$, gdzie $m$ jest liczbą wymiarów wektorów wejściowych a $o$ liczbą wymiarów wektorów wyjściowych. Może być ona używana zarówno do rozwiązywania problemów klasyfikacji, jak i regresji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trenowana funkcja $f$ ma postać kilkuwarstwowej sieci. Każda warstwa złożona jest z pewnej ilości neuronów. Pierwsza warstwa jest _warstwą wejściową_, złożoną z $m$ neuronów. Kolejne warstwy złożone są z pewnej ilości neuronów, których wartości zależą od wartości w poprzedniej warstwie. Ostatnia warstwa - _warstwa wyjściowa_ - składa się z $o$ neuronów. Warstwy pomiędzy warstwą wejściową i wyjściową nazywamy _warstwami ukrytymi_. Wartość funkcji $f$ dla wektora $x \\in R^m$ jest wartość neuronów wyjściowych dla wartości neuronów wejściowych równej $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/multilayerperceptron_network.png\" style=\"width: 350px; text-align: left\" />\n",
    "Rysunek 1: sieć multi-layer perceptron (źródło: https://scikit-learn.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wartość neuronu w warstwie wejściowej zależy od danych wejściowych. Wartość neuronu w warstwach ukrytych i warstwie wyjściowej zależy od sumy ważonej (kombinacji liniowej) wartości neuronów i dodatkowej wartości - bias'u. Dodatkowo, najlepiej aby wartość neuronu $\\in [0, 1]$. W tym celu:\n",
    "* dla _warstwy wejściowej_ - normalizujemy dane\n",
    "* dla _warstw ukrytych_ i _warstwy wyjściowej_ - obliczoną sumę przepuszczamy przez __funkcję aktywacji__, której dziedziną jest $R$ a zbiorem wartości jest $[0, 1]$ . Przykładową funkcją aktywacji jest sigmoid: $f(x) = (1 + e^{-x})^{-1}$. Wykresy funkcji aktywacji znajdują się poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot(f, title):\n",
    "    x = np.linspace(-10, 10, 100)\n",
    "    y = [f(i) for i in x]\n",
    "    plt.plot(x, y)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "plot(lambda x: x, 'identity')\n",
    "plot(lambda x: 1 / (1 + math.exp(-x)), 'sigmoid')\n",
    "plot(lambda x: math.tanh(x), 'tanh')\n",
    "plot(lambda x: max(0, x), 'relu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykresy funkcji aktywacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy zauważyć, że obliczenie wartości neuronów dla warstwy składa się z dwóch kroków:\n",
    "1. _przekształcenie affiniczne_ wektora wartości neuronów z poprzedniej warstwy. Przekształcenie affiniczne jest to przekształcenie w postaci $x \\rightarrow f(x) + b$, gdzie f jest przekształceniem liniowym a b wektorem przesunięcia.\n",
    "2. obliczenie wartości poszczególnych neuronów na podstawie wartości wektora uzyskanych w punkcie (1) z użyciem funkcji aktywacji\n",
    "\n",
    "Przekształcenie affiniczne wektora $X$ w $X'$ możemy zamienić w przekształcenie liniowe, dodając do wektora X dodatkową współrzędną o wartości 1. Umożliwia to dodanie stałego współczynnika do kombinacji liniowych - mnożąc wagę razy 1 otrzymamy zawsze daną wagę (patrz rysunek 1). Dzięki temu, możemy przedstawić współczynniki w postaci macierzy przekształcenia liniowego, a samo przekształcenie zrealizować jako mnożenie macierzy przez wektor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Trenowanie sieci\n",
    "Parametrami funkcji f, które trenujemy, są:\n",
    "* wagi średniej ważonej (`coefs_`) - dla każdego neuronu w warstwie $l$ wag jest tyle, ile neuronów w warstwie $l - 1$,\n",
    "* bias'y (`intercepts_`) - po jednym dla każdego neuronu.\n",
    "\n",
    "Jeśli ilość neuronów w warstwie l oznaczymy jako $n_l$, dla każdej warstwy mamy $(n_{l - 1} + 1) n_l$ wartości do wyznaczenia. Jeśli mamy L warstw w naszej sieci MLP, mamy $\\sum_{l=2}^{L} ((n_{l - 1} + 1) n_l)$ wartości do wyznaczenia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trenowanie sieci neuronowej polega na minimalizacji funkcji strat (loss function) na danym zbiorze treningowym, wyrażającej \"koszt\" sieci neuronowej. Standardowo używa się sumy błędów średniokwadratowych pomiędzy wynikami sieci neuronowej a wartościami oczekiwanymi dla kolejnych wektorów wejściowych. Parametrami funkcji loss są wszystkie parametry (wagi średnich ważonych oraz bias'y) naszej sieci neuronowej. Ilość tych parametrów jest bardzo duża, dlatego nie stosuje się standardowej metody wyznaczania ekstremów funkcji wielu zmiennych (przyrównywanie pochodnych cząstkowych do zera). Zamiast tego, istnieją specjalne metody:\n",
    "* `sgd` - schotastic gradient descent\n",
    "* `adam` - another schotastic gradient descent\n",
    "* `lbfgs` - another optimizer\n",
    "\n",
    "Więcej informacji na temat tych metod znajduje się na wykładzie oraz prezentacji do tego tutoriala.\n",
    "\n",
    "Na kanale 3blue3brown w serwisie YouTube znajduje się 4-odcinkowa seria ilustrująca działanie sieci neuronowej MLP z użyciem schotastic gradient descent na zbiorze MNIST - https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Przykład\n",
    "Na potrzeby tego tutoriala, wykorzystamy bibliotekę `scikit-learn`. Wykorzystuje ona jedynie CPU, przez co nie nadaje się do zastosowań o dużej skali. Obliczenia związane z sieciami neuronowymi dziś wykorzystuje się zazwyczaj z użyciem GPU lub specjalizowanych układów stworzonych do tego celu (np. opartych na FPGA lub będących częściami nowoczesnych układów SoC w smartfonach). Biblioteki takie jak `tensorflow`, `keras` bądź `pytorch` umożliwiają działania na sieciach neuronowych z wykorzystaniem GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Działanie sieci MLP pokażemy standardowo na zbiorze `MNIST`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # korzystamy jedynie w celu pobrania zbioru MNIST\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1] * X_train.shape[2]))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1] * X_test.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biblioteka `scikit-learn` udostępnia klasę `MLPClassifier`, implementującą algorytm MLP używający backpropagation do trenowania.\n",
    "\n",
    "Konstruktor klasy `MLPClassifier` pozwala na ustawienie wielu parametrów, ich pełna lista dostępna jest w dokumentacji `scikit-learn`. Najważniejsze z nich to:\n",
    "* __solver__ - metoda użyta do trenowania sieci. Dostępne metody: `\"sgd\"`\\\\`\"adam\"`\\\\`\"lbfgs\"`\n",
    "* __hidden_layer_sizes__ -  lista rozmiarów warstw ukrytych\n",
    "* __activation__ - użyta funkcja aktywacji. Dostępne funkcje aktywacji:\n",
    "    * `'identity'` - funkcja tożsamościowa\n",
    "    * `'logistic'` - sigmoid\n",
    "    * `'tanh'` - tangens hiperboliczny\n",
    "    * `'relu'` - $f(x) = max(0, x)$\n",
    "* __alpha__ - wartość parametru regularyzacji - pozwala na ograniczanie rozmiaru wag. Odpowiednie dobranie tego parametru pozwala na ograniczenie overfittingu/underfittingu.\n",
    "* __random_state__ - seed funkcji losowej, użytej do generowania wag. Aby wszystkie sieci miały początkowo takie same wagi, wartość parametru random_state musi być jednakowa.\n",
    "* __max_iter__ - maksymalna ilość iteracji trenowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasy `MLPClassifier` używamy podobnie, jak innych klasyfikatorów będących częścią biblioteki sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(\n",
    "    solver='sgd', \n",
    "    hidden_layer_sizes=(50,), \n",
    "    random_state=SEED, \n",
    "    alpha=1e-5, \n",
    "    verbose=10,\n",
    "    max_iter=100\n",
    ")\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Używając zbioru testowego MNIST sprawdźmy, ile wynosi accuracy score naszego modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "y_test\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obiekty klasy `MLPClassifier` udostępniają m. in. następujące atrybuty:\n",
    "* `loss_` - obecna wartość funkcji loss\n",
    "* `coefs_` - lista macierzy wag dla poszczególnych warstw\n",
    "* `intercepts_` - lista wektorów bias dla poszczególnych warstw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyświetlmy wartości współczynników macierzy wag dla niektórych neuronów w warstwie ukrytej w postacji obrazków:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    coefs = clf.coefs_[0][:,i]\n",
    "    coefs = coefs.reshape(28, 28)\n",
    "    plt.imshow(coefs, cmap='Greys')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy zauważyć, że dla _większości_ obrazków wagi macierzy aktywacji nie reprezentują konkretnych kształtów.\n",
    "\n",
    "Wyświetlmy dodatkowo wartości bias dla tych neuronów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.intercepts_[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4) Zadanie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pokazać jak zmiany parametrów sieci (wielkość sieci dobrać do wielkości danych i możliwości obliczeniowych) tj.\n",
    "optymalizator, regularyzacja (np., drop-out), funkcja aktywacji i inne wpływa na wynik.\n",
    "Ważne: __sieć ma być trenowana dla identycznych wag początkowych__ (należy użyć sieci o takim samym rozmiarze i zanicjować sieć z użyciem takiego samego seed'a (parametr `random_state`). \n",
    "W sprawozdaniu należy umieścić tabelkę zawierającą rezultaty i wnioski."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| solver | hidden_layers_sizes | activation | alpha | (other params...) | loss | accuracy_score |\n",
    "|--------|---------------------|------------|-------|-------------------|------|----------------|\n",
    "|        |                     |            |       |                   |      |                |\n",
    "|        |                     |            |       |                   |      |                |\n",
    "|        |                     |            |       |                   |      |                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rozwiązanie zadania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Wpływ długości uczenia sieci na błąd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zmierzmy dokładność naszej sieci w zależności od długości uczenia. W przypadku stochastycznych solverów (sgd, adam) parametr max_iter oznacza liczbę epok. Jeżeli chcemy użyć innego solvera musimy wyznaczyć odpowiednie batch_size oraz max_iter. Czym jest batch size oraz iteracja w ogólności: https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1] * X_train.shape[2]))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1] * X_test.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "X_train, y_train = resample(X_train, y_train, random_state=0, n_samples=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "start_iter = 10 # startowa ilość iteracji\n",
    "max_iters = 301  # maksymalna ilość iteracji\n",
    "step_size = 10   \n",
    "\n",
    "scores = pd.DataFrame(columns=['n_iters', 'test_score', 'loss'])\n",
    "\n",
    "for max_iter in range(start_iter, max_iters + 1, step_size):\n",
    "    clf = MLPClassifier(activation='relu', \n",
    "                        hidden_layer_sizes=(512, 256, 128,), # ity element odpowiada wielkości itej warstwy\n",
    "                        solver='sgd',\n",
    "                        random_state=0, \n",
    "                        max_iter=max_iter, # w przypadku solvera innego niż sgd lub adam trzeba odpowiednio przeliczyć ilość iteracji oraz dodać parametr batch_size\n",
    "                        learning_rate='adaptive',\n",
    "                        alpha=1e-4\n",
    "                       )\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "\n",
    "    row = {\n",
    "        'n_iters': clf.n_iter_,\n",
    "        'max_iter': max_iter,\n",
    "        'train_score': accuracy_score(y_train, y_train_pred),\n",
    "        'test_score': accuracy_score(y_test, y_pred),\n",
    "        'loss': clf.loss_\n",
    "    }\n",
    "    scores = scores.append(row, ignore_index=True)\n",
    "                    \n",
    "scores\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najpierw sprawdźmy jak wygląda ```Accuracy Score``` na zbiorze testowym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.xlabel(\"Number of steps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy on test set\")\n",
    "plt.plot(scores['max_iter'], scores['test_score'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po ~70 krokach ```Accuracy Score``` przestaje rosnąć. Sprawdźmy zatem jak wygląda ```Accuracy Score``` na zbiorze trzeningowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores['max_iter'], scores['train_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przypadku zbioru testowego model osiąga prawie 100% skuteczność na samym sobie po ~40 krokach. Po 70 krokach model zna siebie na pamięć przez co nie może się niczego więcej nauczyć. Następuje underfitting- potrzebujemy zwiększyć zbiór treningowy, żeby osiągnął lepsze wyniki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sieci konwolucyjne - Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sieci konwolucyjne to podklasa sieci neuronowych. Z natury świetnie nadają się do klasyfikacji obrazów oraz wyszukiawania wzroca w obrazie. Ich budowę można podzielić na dwie głównie warstwy:\n",
    "- warstwa konwolucji\n",
    "- warstwa sieci neuronowej\n",
    "\n",
    "\n",
    "Warstwa sieci neuronowej to Fully Connected Network, czyli poznana wcześniej sieć MLP.  \n",
    "Warstwa konwolucji składa się z 3 etapów:\n",
    "1. Konwolucji właściwej\n",
    "2. Operacji ReLU (Rectified Linear Unit)\n",
    "3. Poolingu\n",
    "\n",
    "\n",
    "Warstwa konwolucji może być powtórzona wielokrotnie.\n",
    "Większość frameworków posiada gotowe narzędzia do budowy sieci konwolucyjnych. Poniżej spróbujemy zbudować warstwę konwolucji od zera, aby zobaczyć na czym to polega.\n",
    "\n",
    "![](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-07-at-4-59-29-pm.png?w=748)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Konwolucja\n",
    "\n",
    "Operacja konwolucji jest operacją filtrowania (splotu). Operacja ta przyjmuje na wejściu dwie macierze:\n",
    "\n",
    "- Macierz $I_{MxN}$ reprezentującą obraz. Dla uproszczenia rozważamy jedynie obrazy w 256 odcieniach szarości. Każdy element macierzy $I$ jest więc liczbą całkowitą z zakresu 0 do 255.\n",
    "- Macierz $K_{c×c}$, gdzie $c < M$ oraz $c < N$ reprezentującą filtr. Elementami tej macierzy są liczby zmiennoprzecinkowe (często ujemne).\n",
    "\n",
    "Operacja filtrowania tworzy nowy obraz $J$, którego piksele mają wartość:\n",
    "\n",
    "$J[x,y]=s_{x, y}$\n",
    "\n",
    "\n",
    "$s_{x,y}=\\sum_{i=1}^{c} \\sum_{j=1}^{c}I[x + i - ceil(c/2), y + j - ceil(c/2)]∗k[i,j]$\n",
    "\n",
    "Powstawły obraz będzie odpowiednio mniejszy, zależnie od wielkości $c$. Indeksowanie w powyższych wzorach zaczyna się od 1.\n",
    "\n",
    "Konwolucja jest operacją liniową wykonywaną na macierzy, polegającą na aplikacji pewnego filtru, zwanego kernelem. \n",
    "![](https://ujwlkarn.files.wordpress.com/2016/07/convolution_schematic.gif?w=268&h=196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = X_train[3].reshape(28, 28)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zadeklarujmy filtry pierwszej wartswy konwolucyjnej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_filter = np.zeros((2,3,3))\n",
    "l1_filter[0, :, :] = np.array([[[-1, 0, 1],   \n",
    "                                   [-1, 0, 1],   \n",
    "                                   [-1, 0, 1]]])  \n",
    "l1_filter[1, :, :] = np.array([[[1,   1,  1],   \n",
    "                                   [0,   0,  0],   \n",
    "                                   [-1, -1, -1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(img, conv_filter):\n",
    "    if len(img.shape) != len(conv_filter.shape) - 1: # Check whether number of dimensions is the same\n",
    "        print(\"Error: Number of dimensions in conv filter and image do not match.\")  \n",
    "        return \n",
    "    if len(img.shape) > 2 or len(conv_filter.shape) > 3: # Check if number of image channels matches the filter depth.\n",
    "        if img.shape[-1] != conv_filter.shape[-1]:\n",
    "            print(\"Error: Number of channels in both image and filter must match.\")\n",
    "            return\n",
    "    if conv_filter.shape[1] != conv_filter.shape[2]: # Check if filter dimensions are equal.\n",
    "        print('Error: Filter must be a square matrix. I.e. number of rows and columns must match.')\n",
    "        return\n",
    "    if conv_filter.shape[1]%2==0: # Check if filter diemnsions are odd.\n",
    "        print('Error: Filter must have an odd size. I.e. number of rows and columns must be odd.')\n",
    "        return\n",
    "\n",
    "    # An empty feature map to hold the output of convolving the filter(s) with the image.\n",
    "    feature_maps = np.zeros((img.shape[0]-conv_filter.shape[1]+1, \n",
    "                                img.shape[1]-conv_filter.shape[1]+1, \n",
    "                                conv_filter.shape[0]))\n",
    "\n",
    "    # Convolving the image by the filter(s).\n",
    "    for filter_num in range(conv_filter.shape[0]):\n",
    "        curr_filter = conv_filter[filter_num, :] # getting a filter from the bank.\n",
    "        \"\"\" \n",
    "        Checking if there are mutliple channels for the single filter.\n",
    "        If so, then each channel will convolve the image.\n",
    "        The result of all convolutions are summed to return a single feature map.\n",
    "        \"\"\"\n",
    "        if len(curr_filter.shape) > 2:\n",
    "            conv_map = conv_(img[:, :, 0], curr_filter[:, :, 0]) # Array holding the sum of all feature maps.\n",
    "            for ch_num in range(1, curr_filter.shape[-1]): # Convolving each channel with the image and summing the results.\n",
    "                conv_map = conv_map + conv_(img[:, :, ch_num], \n",
    "                                  curr_filter[:, :, ch_num])\n",
    "        else: # There is just a single channel in the filter.\n",
    "            conv_map = conv_(img, curr_filter)\n",
    "        feature_maps[:, :, filter_num] = conv_map # Holding feature map with the current filter.\n",
    "    return feature_maps # Returning all feature maps.\n",
    "\n",
    "def conv_(img, conv_filter):\n",
    "    filter_size = conv_filter.shape[1]\n",
    "    result = np.zeros((img.shape))\n",
    "    #Looping through the image to apply the convolution operation.\n",
    "    for r in np.uint16(np.arange(filter_size/2.0, \n",
    "                          img.shape[0]-filter_size/2.0+1)):\n",
    "        for c in np.uint16(np.arange(filter_size/2.0, \n",
    "                                           img.shape[1]-filter_size/2.0+1)):\n",
    "            curr_region = img[r-np.uint16(np.floor(filter_size/2.0)):r+np.uint16(np.ceil(filter_size/2.0)), \n",
    "                              c-np.uint16(np.floor(filter_size/2.0)):c+np.uint16(np.ceil(filter_size/2.0))]\n",
    "            #Element-wise multipliplication between the current region and the filter.\n",
    "            curr_result = curr_region * conv_filter\n",
    "            conv_sum = np.sum(curr_result) #Summing the result of multiplication.\n",
    "            result[r, c] = conv_sum #Saving the summation in the convolution layer feature map.\n",
    "            \n",
    "    #Clipping the outliers of the result matrix.\n",
    "    final_result = result[np.uint16(filter_size/2.0):result.shape[0]-np.uint16(filter_size/2.0), \n",
    "                          np.uint16(filter_size/2.0):result.shape[1]-np.uint16(filter_size/2.0)]\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "l1_feature_map = conv(img, l1_filter)\n",
    "plt.imshow(l1_feature_map[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(l1_feature_map[:, :, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ReLU\n",
    "\n",
    "Operacja konwolucji jest liniowa (mnożenie macierzy oraz sumwanie), co niekonieczenie jest przez nas porządane przy uczeniu sieci neuronowej, gdzie dane wejściowe nie są liniowe. Z pomocą przchodzi prosta funkcja ReLU opisana wzorem $Output = max(0, Input)$, gdzie $Input$ to wartość piksela wynikowej macierzy po zaaplikowaniu filtrów\n",
    "![](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-10-at-2-23-48-am.png?w=537&h=168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(feature_map):  \n",
    "    relu_out = np.zeros(feature_map.shape)  \n",
    "    for map_num in range(feature_map.shape[-1]):  \n",
    "        for r in np.arange(0,feature_map.shape[0]):  \n",
    "            for c in np.arange(0, feature_map.shape[1]):  \n",
    "                relu_out[r, c, map_num] = np.max([feature_map[r, c, map_num], 0])\n",
    "    return relu_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l1_feature_map_relu = relu(l1_feature_map)\n",
    "plt.imshow(l1_feature_map_relu[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(l1_feature_map_relu[:, :, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inną funkcją do konwersji danych w nieliniowe, jest poznana wcześniej funkcja sigmoid, jednak ReLU uznawane jest za najlepszą ze względu na wyniki poparte badaniami."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Pooling\n",
    "\n",
    "Pooling zwany również subsampling lub downsampling jest operacją zmniejszającą ilość rozmiar otrzymanej feature mapy. Rozpatruje się 3 głównie metody pooling:\n",
    "- Max\n",
    "- Average\n",
    "- Sum\n",
    "\n",
    "Metod tych może być oczywiście więcej.\n",
    "\n",
    "Operacja jest dosyć intuicyjna. Dzielimy Macierz na podmacierze, na których aplikujemy funkcję poolingu, która zwróci nam w zależności od wybranej metody skalar. W ten sposób uzyskamy mniejszą macież.\n",
    "![](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-10-at-3-38-39-am.png?w=494)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(feature_map, size=2, stride=2):\n",
    "    #Preparing the output of the pooling operation.\n",
    "    pool_out = np.zeros((np.uint16((feature_map.shape[0]-size+1)/stride+1),\n",
    "                            np.uint16((feature_map.shape[1]-size+1)/stride+1),\n",
    "                            feature_map.shape[-1]))\n",
    "    for map_num in range(feature_map.shape[-1]):\n",
    "        r2 = 0\n",
    "        for r in np.arange(0,feature_map.shape[0]-size+1, stride):\n",
    "            c2 = 0\n",
    "            for c in np.arange(0, feature_map.shape[1]-size+1, stride):\n",
    "                pool_out[r2, c2, map_num] = np.max([feature_map[r:r+size,  c:c+size, map_num]])\n",
    "                c2 = c2 + 1\n",
    "            r2 = r2 +1\n",
    "    return pool_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_feature_map_relu_pool = pooling(l1_feature_map_relu, 2, 2)\n",
    "plt.imshow(l1_feature_map_relu_pool[:, :, 0])\n",
    "l1_feature_map_relu_pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(l1_feature_map_relu_pool[:, :, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na koniec możemy dodać kolejne warstwy konwolucyjne. Z powodu niwielkich rozmiarów obrazka ograniczymy się do dwóch warstw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_feature_map_relu_pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l2_filter = np.random.rand(16, 3, 3, l1_feature_map_relu_pool.shape[-1])\n",
    "l2_feature_map = conv(l1_feature_map_relu_pool, l2_filter)\n",
    "l2_feature_map_relu = relu(l2_feature_map)\n",
    "l2_feature_map_relu_pool = pooling(l2_feature_map_relu, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(l2_feature_map_relu_pool[:, :, 0])\n",
    "l2_feature_map_relu_pool.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 CNN w Tensorflow\n",
    "\n",
    "Powyższe kroki wykorzystamy do stworzenia sieci CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, utils\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "# Pierwsza warstwa konwolucyjna\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Druga warstwa konwolucyjna\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Trzecia warstwa konwolucyjna\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Zamiana trójwymiarowej macierzy na jednowymiarowy wektor (przestawienie kolejnych wierszy kolumn liniowo)\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Sieć neuronowa\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Zadanie dla studentów\n",
    "\n",
    "Pokazać jak zmiany parametrów sieci (wielkość sieci dobrać do wielkości danych i możliwości obliczeniowych) tj.\n",
    "ilość wartsw konwolucyjnych, wielkości kerneli, metoda poolingu.\n",
    "Ważne: __sieć ma być trenowana dla identycznych wag początkowych__ (należy użyć sieci o takim samym rozmiarze i zanicjować sieć z użyciem takiego samego seed'a (parametr `random_state`). \n",
    "W sprawozdaniu należy umieścić tabelkę zawierającą rezultaty i wnioski."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| kernel_sizes | activation | pooling | (other params...) | loss | accuracy_score |\n",
    "|---------------------|------------|-------|-------------------|------|----------------|\n",
    "|                     |            |       |                   |      |                |\n",
    "|        |                     |            |       |                   |      |                |\n",
    "|        |                     |            |       |                   |      |                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Zadanie dla studentów\n",
    "\n",
    "Wnioski z porównania sieci MLP i CNN.\n",
    "Która otrzymuje lepsze wyniki i dlaczego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Zadanie dla studentów\n",
    "\n",
    "Na zbiorze FMNIST i 20NG (TNG) (można dokonać zmniejszenia ilości wymiarów używając\n",
    "PCA) porównać performance sieci MLP i CNN w funkcji ilości warstw. Zaczynamy od\n",
    "jednowarstwowej sieci MLP i minimalnej sieci CNN o porównywalnych ilościach wag.\n",
    "Następnie zwiększamy liczbę warstw zakładając, że ilość wag jest podobna jak dla\n",
    "początkowej konfiguracji. WNIOSKI.\n",
    "Wyniki dokładności porównać do najlepszego modelu klasycznego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
